👉 Type: Static Array

1. Insert at End

✨ Flashcard
"Place at arr[length] → increment length."

⚡ Full Approach
Steps:

Check if there’s capacity (length < capacity).
Insert the new value at arr[length].
Return updated length (length + 1).

⏱️ Complexity

Time: O(1) → Direct insertion.
Space: O(1) → No extra memory.

2. Remove from End

✨ Flashcard
"Clear arr[length-1] → decrement length."

⚡ Full Approach
Steps:

Check if the array is non-empty (length > 0).
Reset last element (arr[length-1] = 0).
Return updated length (length - 1).

⏱️ Complexity

Time: O(1) → Direct removal.
Space: O(1) → No extra memory.

3. Insert in Middle

✨ Flashcard
"Shift right → place value → increment length."

⚡ Full Approach
Steps:

Start from the last real element (length - 1) down to i.
Shift each element one step to the right (arr[index + 1] = arr[index]).
Insert the value at position i.
Return updated length (length + 1).

⏱️ Complexity

Time: O(n) → Shifting elements.
Space: O(1) → No extra memory.

4. Remove from Middle

✨ Flashcard
"Shift left → clear last slot → decrement length."

⚡ Full Approach
Steps:

Start from i+1 to the end (length - 1).
Shift elements one step to the left (arr[index-1] = arr[index]).
Reset last slot (arr[length-1] = 0).
Return updated length (length - 1).

⏱️ Complexity

Time: O(n) → Shifting elements.
Space: O(1) → No extra memory.

5. Print Array

✨ Flashcard
"Loop from 0 → capacity → print each value."

⚡ Full Approach
Steps:

Loop through 0 to capacity-1.
Print each arr[i].

⏱️ Complexity

Time: O(n) → Iterate through array.

Space: O(1) → No extra memory.

👉 Type: Stack (Dynamic Array)

1. Push (Insert at Top)

✨ Flashcard
"Append to list → top grows upward."

⚡ Full Approach
Steps:

Call append(n) to add element at the end of the list.
The new element becomes the top of the stack.

⏱️ Complexity

Time: O(1) → Direct append.
Space: O(1) → No extra memory (except element storage).

2. Pop (Remove from Top)

✨ Flashcard
"Pop last → return top element."

⚡ Full Approach
Steps:

Call pop() to remove the last element.
Return the removed element (the previous top).

⏱️ Complexity

Time: O(1) → Direct pop from end.
Space: O(1) → No extra memory.

👉 Type: Singly Linked List

1. Insert Node at End

✨ Flashcard
"Create new_node → link tail.next → move tail."

⚡ Full Approach
Steps:

Create a new node (new_node = ListNode(val)).
Link the current tail node to the new node (self.tail.next = new_node).
Move self.tail to point to the new node (self.tail = new_node).

⏱️ Complexity

Time: O(1) → Direct tail reference.
Space: O(1) → No extra memory.

2. Remove Node at Given Index

✨ Flashcard
"Traverse to node before target → skip target → update tail if last node removed."

⚡ Full Approach
Steps:

Start from the dummy head node (curr = self.head).
Use a for loop to move curr forward index times.
If curr.next is None, index is out of range → exit function.

Once at the node before the target, update link:
curr.next = curr.next.next

If we removed the last node, update:
self.tail = curr

⏱️ Complexity

Time: O(n) → Traverses up to index.
Space: O(1) → In-place modification.

👉 Type: Doubly Linked List

1. Insert at Front
Key principle When inserting a node:
Link the new node to the existing nodes first. Then update the dummy node (head or tail) to point to the new node.

✨ Flashcard
"Create a new node → link between head and first real node."

head -> 5 -> 10 -> 20 -> tail       think with this example list for easier visualization

⚡ Full Approach
Steps:

Create a new_node with the value.
Set new_node.prev = head.
Set new_node.next = head.next (the old first node).
Update the old first node’s prev to point to new_node.
Update head.next to point to new_node.

⏱️ Complexity

Time: O(1) → Direct pointer updates.
Space: O(1) → No extra memory.

2. Insert at End

✨ Flashcard
"Create a new node → link between last node and tail."

head -> 5 -> 10 -> 20 -> tail

⚡ Full Approach
Steps:

Create a new_node with the value.
Set new_node.next = tail.
Set new_node.prev = tail.prev (the old last node).
Update the old last node’s next to point to new_node.
Update tail.prev to point to new_node.

⏱️ Complexity

Time: O(1)
Space: O(1)

3. Remove from Front
Key principle When removing a node:
Reconnect the previous and next nodes to each other first, then detach the target node.

✨ Flashcard
"Skip the first node by connecting head to the second node."

head -> 5 -> 10 -> 20 -> tail


⚡ Full Approach
Steps:

Check if the list is empty (head.next == tail).
If empty, print "List is empty" and return.
Update head.next to point to the second node.
Update the second node’s prev to point to head.

⏱️ Complexity

Time: O(1)
Space: O(1)

4. Remove from End

✨ Flashcard
"Skip the last node by connecting tail to the second-last node."

head -> 5 -> 10 -> 20 -> tail

⚡ Full Approach
Steps:

Check if the list is empty (head.next == tail).
If empty, print "List is empty" and return.
Update tail.prev to point to the second-last node.
Update the second-last node’s next to point to tail.

⏱️ Complexity

Time: O(1)
Space: O(1)

5. Print List

✨ Flashcard
"Traverse from head to tail, printing each value."

⚡ Full Approach
Steps:

Start at the node after head.
Keep moving forward using next.
Print each node’s value until reaching tail.

⏱️ Complexity

Time: O(n) → Must traverse all nodes.
Space: O(1) → No extra memory.

🔹 When to use deque vs list?
Use deque when you need fast appends/pops from both ends (O(1) time).
Use list when you need fast random access (indexing, slicing).

👉 Type: Queue (Linked List with Dummy Node)

1. Enqueue (Insert at Tail)

✨ Flashcard
"Create a new node → attach to tail.next → move tail pointer."

⚡ Full Approach
Steps:
Create a new node with the given value.
Link the current tail.next to this new node.
Update tail to point to the new node.

⏱️ Complexity

Time: O(1) → Constant-time insertion at the end.
Space: O(1) → No extra memory besides the new node.

2. Dequeue (Remove from Head)

✨ Flashcard
"Take value from head.next → move head.next forward → reset tail if empty."

⚡ Full Approach

Steps:
If head.next is None, the queue is empty → return None.
Store the value from head.next.val.
Update head.next to skip the removed node (head.next.next).
If the queue is now empty, reset tail = head.
Return the stored value.

⏱️ Complexity

Time: O(1) → Constant-time removal from the front.
Space: O(1) → No extra memory.

👉 Type: Sorting Algorithm

1. Insertion Sort

✨ Flashcard:
"Pick next element → compare with sorted part → shift bigger elements → insert at correct position."

⚡ Full Approach
Steps:
Start from the second element (i = 1).
Compare it with elements before it (j = i-1) in the sorted portion.

While the element at j is greater than current element, shift elements to the right:
tmp = arr[j+1]
arr[j+1] = arr[j]
arr[j] = tmp
Decrement j until you find the correct position.

Repeat for all elements (i = 1 → len(arr)-1).

Return the sorted array.
⏱️ Complexity

Time:
Best: O(n) → already sorted
Worst: O(n²) → reverse sorted
Space: O(1) → sorts in place
Stable: Yes → preserves relative order of equal elements

👉 Type: In-Place Sorting (Heapsort)

✨ Flashcard:
"Build max-heap → swap root with end → heapify reduced heap → repeat."

⚡ Full Approach
Steps:
Build a max-heap from the array.
Swap the root (max element) with the last element.
Reduce heap size by 1 and heapify the root.
Repeat until the heap size is 1.
Array becomes sorted in ascending order.

Heapify:
For a node i, compare with left and right children.
Swap with the largest child if necessary.
Continue heapifying down the subtree.

⏱️ Complexity

Time: O(n log n) → building heap + extracting all elements
Space: O(1) extra → in-place sorting
Stable: ❌ Heapsort is not stable

👉 Type: In-Place Sorting (QuickSort)

✨ Flashcard:
"Pick pivot → partition array → recursively sort left and right → done."

⚡ Full Approach
Steps:

Partition:
Pick the last element as pivot.
Rearrange elements so that all smaller elements are on the left, larger on the right.
Return the pivot index after partitioning.

Recursive Sort:
Recursively apply QuickSort to left of pivot and right of pivot.
Base case: if start ≥ end, return.

In-place:
No extra array needed → minimal space
Recursion stack uses O(log n) on average.

⏱️ Complexity
Time: O(n log n)
Average: O(n log n)
Worst (already sorted pivot): O(n²)

Space: O(log n) → recursion stack
Worst-case stack: O(n) → Occurs in worst case pivot choices (highly unbalanced splits).
Stable: ❌ QuickSort is not stable

👉 Type: Divide & Conquer (MergeSort)

✅ Key Idea
Divide: split array into smaller halves recursively.
Conquer: sort the tiny arrays (length 1 is already sorted).
Combine: merge sorted halves back together.

✨ Flashcard:
"Divide array → recursively sort halves → merge sorted halves → done."

⚡ Full Approach
Steps:

Divide:
Split the array into two halves (midpoint).

Recursive Sort:
Recursively apply MergeSort on the left half.
Recursively apply MergeSort on the right half.

Merge:
Merge two sorted halves into one sorted array.
Use two pointers to compare elements from each half and build the sorted result.

Base Case:
If array has 0 or 1 element, return as already sorted.

In-place:
Standard MergeSort is not in-place because merging requires extra space O(n).
There are advanced in-place variants, but usually not used due to complexity.

⏱️ Complexity
Time:
    Best: O(n log n)
    Average: O(n log n)
    Worst: O(n log n)
Space: O(n) (extra arrays for merging)
Stable: ✅ MergeSort is stable

✅ Notes:
Preferred for linked lists (easy to split & merge).
Consistent O(n log n) time regardless of input order.
Heavier on memory compared to QuickSort.

👉 Type: Divide & Conquer (QuickSort)

✅ Key Idea

Divide: pick a pivot and partition array into two groups: smaller than pivot & larger than pivot.

Conquer: recursively sort left and right groups.

Combine: pivot ends up in the right place automatically; nothing to merge.

✨ Flashcard:
"Pick a pivot → put smaller left, bigger right → recursively sort sides → done."

⚡ Full Approach

Steps:
Choose Pivot:
Commonly the last element (arr[end]).

Partition:
Move all elements < pivot to the left.
Track index (partition_index) where pivot should finally sit.
Swap pivot into correct position.

Recursive Sort:
Recursively call QuickSort on the left side (start to partition_index - 1).
Recursively call QuickSort on the right side (partition_index + 1 to end).

Base Case:
If subarray length is 0 or 1, it’s already sorted → stop recursion.

In-place:

✅ QuickSort sorts the array in place (O(1) extra space for swaps, aside from recursion stack).

⚠️ But recursion uses O(log n) space in the average case, O(n) in the worst case.

⏱️ Complexity

Time:
Best: O(n log n) (perfectly balanced partitions).
Average: O(n log n).
Worst: O(n²) (bad pivots, e.g., already sorted array with naive pivot choice).
Space: O(log n) (recursion stack).
Stable: ❌ Not stable (relative order of equal elements may change).

✅ Notes:
Very fast in practice due to good cache behavior & in-place sorting.
Widely used in standard libraries (with optimizations).
Picking a good pivot (e.g., random pivot or median-of-three) avoids worst-case.

👉 Type: Divide & Conquer (3-way QuickSort / Dutch National Flag)

✅ Key Idea

Divide: pick a random pivot and partition array into three groups:

Smaller than pivot
Equal to pivot
Larger than pivot

Conquer: recursively sort only the smaller and larger groups.

Combine: the equal-to-pivot block is already in the right place; nothing to merge.

✨ Flashcard:
"Pick random pivot → smaller left, equal middle, larger right → recursively sort sides → done."

⚡ Full Approach

Steps:

Choose Pivot:
Pick a random element from the current subarray.
Swap it with the last element for convenience.

3-Way Partition:

Use three pointers:

lt → boundary of elements < pivot
i → current element
gt → boundary of elements > pivot

Scan the array:

If nums[i] < pivot → swap with lt, move both lt and i forward.
If nums[i] > pivot → swap with gt, move gt backward (check new element at i).
If nums[i] == pivot → just move i forward.

Recursive Sort:

Recursively sort left section (start to lt-1)
Recursively sort right section (gt+1 to end)

Base Case:

Stop recursion if subarray has 0 or 1 element.

In-place:
Sorting happens inside the original array.
Extra space: O(log n) for recursion stack.

⏱️ Complexity

Time:
Best: O(n log n) (balanced partitions)
Average: O(n log n)
Worst: O(n²) (extremely rare with random pivot)

Space: O(log n) recursion stack

Stable: ❌ Not stable (equal elements may change order)

✅ Notes:

Handles many duplicates efficiently (no TLE).
Random pivot ensures good average performance.
In-place sorting → memory-efficient.

👉 Type: Comparison-Based (Bubble Sort)

✅ Key Idea
Repeatedly compare adjacent elements and swap them if they are in the wrong order.
With each pass, the largest element bubbles up to the end of the list.

✨ Flashcard:
"Compare neighbors → swap if out of order → largest element moves to the end each pass → repeat until sorted."

⚡ Full Approach

Steps:
Outer Loop (i):
Runs n passes at most.
After each pass, the last element(s) are in correct position.

Inner Loop (j):
for j in range(0, n-i-1)
Compares each pair arr[j] and arr[j+1].
If arr[j] > arr[j+1], swap them.

Swapped Flag:
If no swaps occur in a full pass, the array is already sorted → stop early.

Base Case:
If array length is 0 or 1, it’s already sorted.

⏱️ Complexity
Time:
Best: O(n) (already sorted, only one pass).
Average: O(n²).
Worst: O(n²).

Space:
O(1) → in-place sorting.

Stable: ✅ Yes (equal elements keep original order).

✅ Notes:
Very simple, but inefficient for large arrays.
Good for teaching, not for production.
Useful when the array is almost sorted (best case O(n)).

👉 Type: Counting-Based (Special Case of Counting Sort)

✅ Key Idea
Count how many times each number appears (0, 1, or 2).
Then overwrite the original array by filling in 0s, then 1s, then 2s, according to their counts.

✨ Flashcard:
"Count frequencies → rewrite array with 0s, then 1s, then 2s → sorted!"

⚡ Full Approach

Steps:

Initialize Counts:
Create a counter list with three slots for 0, 1, and 2.

Count Elements:
Traverse the array and record how many times each number occurs.

Example:
Input → [2, 0, 2, 1, 1, 0]
Counts → [2, 2, 2] (2 zeros, 2 ones, 2 twos).

Refill Array In-Place:
Use the counts to rewrite the array: first all 0s, then all 1s, then all 2s.

Return Sorted Array:
Final result → [0, 0, 1, 1, 2, 2].

⏱️ Complexity

Time: O(n) → one pass to count, one pass to rebuild.

Space: O(1) → only 3 extra counters.

Stable: ✅ Yes (relative order of equal elements doesn’t change because we just group them).

✅ Notes:

More efficient than comparison-based sorts for this restricted case.
Works only when the array contains 0, 1, and 2.
A generalized version is Counting Sort, which works for any integer range.