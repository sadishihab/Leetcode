👉 Type: Static Array

1. Insert at End

✨ Flashcard
"Place at arr[length] → increment length."

⚡ Full Approach
Steps:

Check if there’s capacity (length < capacity).
Insert the new value at arr[length].
Return updated length (length + 1).

⏱️ Complexity

Time: O(1) → Direct insertion.
Space: O(1) → No extra memory.

2. Remove from End

✨ Flashcard
"Clear arr[length-1] → decrement length."

⚡ Full Approach
Steps:

Check if the array is non-empty (length > 0).
Reset last element (arr[length-1] = 0).
Return updated length (length - 1).

⏱️ Complexity

Time: O(1) → Direct removal.
Space: O(1) → No extra memory.

3. Insert in Middle

✨ Flashcard
"Shift right → place value → increment length."

⚡ Full Approach
Steps:

Start from the last real element (length - 1) down to i.
Shift each element one step to the right (arr[index + 1] = arr[index]).
Insert the value at position i.
Return updated length (length + 1).

⏱️ Complexity

Time: O(n) → Shifting elements.
Space: O(1) → No extra memory.

4. Remove from Middle

✨ Flashcard
"Shift left → clear last slot → decrement length."

⚡ Full Approach
Steps:

Start from i+1 to the end (length - 1).
Shift elements one step to the left (arr[index-1] = arr[index]).
Reset last slot (arr[length-1] = 0).
Return updated length (length - 1).

⏱️ Complexity

Time: O(n) → Shifting elements.
Space: O(1) → No extra memory.

5. Print Array

✨ Flashcard
"Loop from 0 → capacity → print each value."

⚡ Full Approach
Steps:

Loop through 0 to capacity-1.
Print each arr[i].

⏱️ Complexity

Time: O(n) → Iterate through array.

Space: O(1) → No extra memory.

👉 Type: Stack (Dynamic Array)

1. Push (Insert at Top)

✨ Flashcard
"Append to list → top grows upward."

⚡ Full Approach
Steps:

Call append(n) to add element at the end of the list.
The new element becomes the top of the stack.

⏱️ Complexity

Time: O(1) → Direct append.
Space: O(1) → No extra memory (except element storage).

2. Pop (Remove from Top)

✨ Flashcard
"Pop last → return top element."

⚡ Full Approach
Steps:

Call pop() to remove the last element.
Return the removed element (the previous top).

⏱️ Complexity

Time: O(1) → Direct pop from end.
Space: O(1) → No extra memory.

👉 Type: Singly Linked List

1. Insert Node at End

✨ Flashcard
"Create new_node → link tail.next → move tail."

⚡ Full Approach
Steps:

Create a new node (new_node = ListNode(val)).
Link the current tail node to the new node (self.tail.next = new_node).
Move self.tail to point to the new node (self.tail = new_node).

⏱️ Complexity

Time: O(1) → Direct tail reference.
Space: O(1) → No extra memory.

2. Remove Node at Given Index

✨ Flashcard
"Traverse to node before target → skip target → update tail if last node removed."

⚡ Full Approach
Steps:

Start from the dummy head node (curr = self.head).
Use a for loop to move curr forward index times.
If curr.next is None, index is out of range → exit function.

Once at the node before the target, update link:
curr.next = curr.next.next

If we removed the last node, update:
self.tail = curr

⏱️ Complexity

Time: O(n) → Traverses up to index.
Space: O(1) → In-place modification.

👉 Type: Doubly Linked List

1. Insert at Front
Key principle When inserting a node:
Link the new node to the existing nodes first. Then update the dummy node (head or tail) to point to the new node.

✨ Flashcard
"Create a new node → link between head and first real node."

head -> 5 -> 10 -> 20 -> tail       think with this example list for easier visualization

⚡ Full Approach
Steps:

Create a new_node with the value.
Set new_node.prev = head.
Set new_node.next = head.next (the old first node).
Update the old first node’s prev to point to new_node.
Update head.next to point to new_node.

⏱️ Complexity

Time: O(1) → Direct pointer updates.
Space: O(1) → No extra memory.

2. Insert at End

✨ Flashcard
"Create a new node → link between last node and tail."

head -> 5 -> 10 -> 20 -> tail

⚡ Full Approach
Steps:

Create a new_node with the value.
Set new_node.next = tail.
Set new_node.prev = tail.prev (the old last node).
Update the old last node’s next to point to new_node.
Update tail.prev to point to new_node.

⏱️ Complexity

Time: O(1)
Space: O(1)

3. Remove from Front
Key principle When removing a node:
Reconnect the previous and next nodes to each other first, then detach the target node.

✨ Flashcard
"Skip the first node by connecting head to the second node."

head -> 5 -> 10 -> 20 -> tail


⚡ Full Approach
Steps:

Check if the list is empty (head.next == tail).
If empty, print "List is empty" and return.
Update head.next to point to the second node.
Update the second node’s prev to point to head.

⏱️ Complexity

Time: O(1)
Space: O(1)

4. Remove from End

✨ Flashcard
"Skip the last node by connecting tail to the second-last node."

head -> 5 -> 10 -> 20 -> tail

⚡ Full Approach
Steps:

Check if the list is empty (head.next == tail).
If empty, print "List is empty" and return.
Update tail.prev to point to the second-last node.
Update the second-last node’s next to point to tail.

⏱️ Complexity

Time: O(1)
Space: O(1)

5. Print List

✨ Flashcard
"Traverse from head to tail, printing each value."

⚡ Full Approach
Steps:

Start at the node after head.
Keep moving forward using next.
Print each node’s value until reaching tail.

⏱️ Complexity

Time: O(n) → Must traverse all nodes.
Space: O(1) → No extra memory.

🔹 When to use deque vs list?
Use deque when you need fast appends/pops from both ends (O(1) time).
Use list when you need fast random access (indexing, slicing).

👉 Type: Queue (Linked List with Dummy Node)

1. Enqueue (Insert at Tail)

✨ Flashcard
"Create a new node → attach to tail.next → move tail pointer."

⚡ Full Approach
Steps:
Create a new node with the given value.
Link the current tail.next to this new node.
Update tail to point to the new node.

⏱️ Complexity

Time: O(1) → Constant-time insertion at the end.
Space: O(1) → No extra memory besides the new node.

2. Dequeue (Remove from Head)

✨ Flashcard
"Take value from head.next → move head.next forward → reset tail if empty."

⚡ Full Approach

Steps:
If head.next is None, the queue is empty → return None.
Store the value from head.next.val.
Update head.next to skip the removed node (head.next.next).
If the queue is now empty, reset tail = head.
Return the stored value.

⏱️ Complexity

Time: O(1) → Constant-time removal from the front.
Space: O(1) → No extra memory.

👉 Type: Sorting Algorithm

1. Insertion Sort

✨ Flashcard:
"Pick next element → compare with sorted part → shift bigger elements → insert at correct position."

⚡ Full Approach
Steps:
Start from the second element (i = 1).
Compare it with elements before it (j = i-1) in the sorted portion.

While the element at j is greater than current element, shift elements to the right:
tmp = arr[j+1]
arr[j+1] = arr[j]
arr[j] = tmp
Decrement j until you find the correct position.

Repeat for all elements (i = 1 → len(arr)-1).

Return the sorted array.
⏱️ Complexity

Time:
Best: O(n) → already sorted
Worst: O(n²) → reverse sorted
Space: O(1) → sorts in place
Stable: Yes → preserves relative order of equal elements

👉 Type: In-Place Sorting (Heapsort)

✨ Flashcard:
"Build max-heap → swap root with end → heapify reduced heap → repeat."

⚡ Full Approach
Steps:
Build a max-heap from the array.
Swap the root (max element) with the last element.
Reduce heap size by 1 and heapify the root.
Repeat until the heap size is 1.
Array becomes sorted in ascending order.

Heapify:
For a node i, compare with left and right children.
Swap with the largest child if necessary.
Continue heapifying down the subtree.

⏱️ Complexity

Time: O(n log n) → building heap + extracting all elements
Space: O(1) extra → in-place sorting
Stable: ❌ Heapsort is not stable

👉 Type: In-Place Sorting (QuickSort)

✨ Flashcard:
"Pick pivot → partition array → recursively sort left and right → done."

⚡ Full Approach
Steps:

Partition:
Pick the last element as pivot.
Rearrange elements so that all smaller elements are on the left, larger on the right.
Return the pivot index after partitioning.

Recursive Sort:
Recursively apply QuickSort to left of pivot and right of pivot.
Base case: if start ≥ end, return.

In-place:
No extra array needed → minimal space
Recursion stack uses O(log n) on average.

⏱️ Complexity
Time: O(n log n)
Average: O(n log n)
Worst (already sorted pivot): O(n²)

Space: O(log n) → recursion stack
Worst-case stack: O(n) → Occurs in worst case pivot choices (highly unbalanced splits).
Stable: ❌ QuickSort is not stable

👉 Type: Divide & Conquer (MergeSort)

✅ Key Idea
Divide: split array into smaller halves recursively.
Conquer: sort the tiny arrays (length 1 is already sorted).
Combine: merge sorted halves back together.

✨ Flashcard:
"Divide array → recursively sort halves → merge sorted halves → done."

⚡ Full Approach
Steps:

Divide:
Split the array into two halves (midpoint).

Recursive Sort:
Recursively apply MergeSort on the left half.
Recursively apply MergeSort on the right half.

Merge:
Merge two sorted halves into one sorted array.
Use two pointers to compare elements from each half and build the sorted result.

Base Case:
If array has 0 or 1 element, return as already sorted.

In-place:
Standard MergeSort is not in-place because merging requires extra space O(n).
There are advanced in-place variants, but usually not used due to complexity.

⏱️ Complexity
Time:
    Best: O(n log n)
    Average: O(n log n)
    Worst: O(n log n)
Space: O(n) (extra arrays for merging)
Stable: ✅ MergeSort is stable

✅ Notes:
Preferred for linked lists (easy to split & merge).
Consistent O(n log n) time regardless of input order.
Heavier on memory compared to QuickSort.

👉 Type: Divide & Conquer (QuickSort)

✅ Key Idea

Divide: pick a pivot and partition array into two groups: smaller than pivot & larger than pivot.

Conquer: recursively sort left and right groups.

Combine: pivot ends up in the right place automatically; nothing to merge.

✨ Flashcard:
"Pick a pivot → put smaller left, bigger right → recursively sort sides → done."

⚡ Full Approach

Steps:
Choose Pivot:
Commonly the last element (arr[end]).

Partition:
Move all elements < pivot to the left.
Track index (partition_index) where pivot should finally sit.
Swap pivot into correct position.

Recursive Sort:
Recursively call QuickSort on the left side (start to partition_index - 1).
Recursively call QuickSort on the right side (partition_index + 1 to end).

Base Case:
If subarray length is 0 or 1, it’s already sorted → stop recursion.

In-place:

✅ QuickSort sorts the array in place (O(1) extra space for swaps, aside from recursion stack).

⚠️ But recursion uses O(log n) space in the average case, O(n) in the worst case.

⏱️ Complexity

Time:
Best: O(n log n) (perfectly balanced partitions).
Average: O(n log n).
Worst: O(n²) (bad pivots, e.g., already sorted array with naive pivot choice).
Space: O(log n) (recursion stack).
Stable: ❌ Not stable (relative order of equal elements may change).

✅ Notes:
Very fast in practice due to good cache behavior & in-place sorting.
Widely used in standard libraries (with optimizations).
Picking a good pivot (e.g., random pivot or median-of-three) avoids worst-case.

👉 Type: Divide & Conquer (3-way QuickSort / Dutch National Flag)

✅ Key Idea

Divide: pick a random pivot and partition array into three groups:

Smaller than pivot
Equal to pivot
Larger than pivot

Conquer: recursively sort only the smaller and larger groups.

Combine: the equal-to-pivot block is already in the right place; nothing to merge.

✨ Flashcard:
"Pick random pivot → smaller left, equal middle, larger right → recursively sort sides → done."

⚡ Full Approach

Steps:

Choose Pivot:
Pick a random element from the current subarray.
Swap it with the last element for convenience.

3-Way Partition:

Use three pointers:

lt → boundary of elements < pivot
i → current element
gt → boundary of elements > pivot

Scan the array:

If nums[i] < pivot → swap with lt, move both lt and i forward.
If nums[i] > pivot → swap with gt, move gt backward (check new element at i).
If nums[i] == pivot → just move i forward.

Recursive Sort:

Recursively sort left section (start to lt-1)
Recursively sort right section (gt+1 to end)

Base Case:

Stop recursion if subarray has 0 or 1 element.

In-place:
Sorting happens inside the original array.
Extra space: O(log n) for recursion stack.

⏱️ Complexity

Time:
Best: O(n log n) (balanced partitions)
Average: O(n log n)
Worst: O(n²) (extremely rare with random pivot)

Space: O(log n) recursion stack

Stable: ❌ Not stable (equal elements may change order)

✅ Notes:

Handles many duplicates efficiently (no TLE).
Random pivot ensures good average performance.
In-place sorting → memory-efficient.

👉 Type: Comparison-Based (Bubble Sort)

✅ Key Idea
Repeatedly compare adjacent elements and swap them if they are in the wrong order.
With each pass, the largest element bubbles up to the end of the list.

✨ Flashcard:
"Compare neighbors → swap if out of order → largest element moves to the end each pass → repeat until sorted."

⚡ Full Approach

Steps:
Outer Loop (i):
Runs n passes at most.
After each pass, the last element(s) are in correct position.

Inner Loop (j):
for j in range(0, n-i-1)
Compares each pair arr[j] and arr[j+1].
If arr[j] > arr[j+1], swap them.

Swapped Flag:
If no swaps occur in a full pass, the array is already sorted → stop early.

Base Case:
If array length is 0 or 1, it’s already sorted.

⏱️ Complexity
Time:
Best: O(n) (already sorted, only one pass).
Average: O(n²).
Worst: O(n²).

Space:
O(1) → in-place sorting.

Stable: ✅ Yes (equal elements keep original order).

✅ Notes:
Very simple, but inefficient for large arrays.
Good for teaching, not for production.
Useful when the array is almost sorted (best case O(n)).

👉 Type: Counting-Based (Bucket sort)

✅ Key Idea
Count how many times each number appears (0, 1, or 2).
Then overwrite the original array by filling in 0s, then 1s, then 2s, according to their counts.

✨ Flashcard:
"Count frequencies → rewrite array with 0s, then 1s, then 2s → sorted!"

⚡ Full Approach

Steps:

Initialize Counts:
Create a counter list with three slots for 0, 1, and 2.

Count Elements:
Traverse the array and record how many times each number occurs.

Example:
Input → [2, 0, 2, 1, 1, 0]
Counts → [2, 2, 2] (2 zeros, 2 ones, 2 twos).

Refill Array In-Place:
Use the counts to rewrite the array: first all 0s, then all 1s, then all 2s.

Return Sorted Array:
Final result → [0, 0, 1, 1, 2, 2].

⏱️ Complexity

Time: O(n) → one pass to count, one pass to rebuild.

Space: O(1) → only 3 extra counters.

Stable: ✅ Yes (relative order of equal elements doesn’t change because we just group them).

✅ Notes:

More efficient than comparison-based sorts for this restricted case.
Works only when the array contains 0, 1, and 2.
A generalized version is Counting Sort, which works for any integer range.

👉 Type: Comparison-Based (Heap Sort)

✅ Key Idea
Turn the array into a max heap (largest element always at the root).
Then repeatedly swap the root with the last element, shrink the heap size, and re-heapify until sorted.

✨ Flashcard:
"Build max heap → swap root with last → shrink heap → heapify again → sorted!"

⚡ Full Approach
Steps:

Build Max Heap:
Start from the last non-leaf node (n//2 - 1) and call heapify upwards.
After this step, the largest element will be at index 0.

Extract Maximum One by One:
Swap nums[0] (root, largest) with nums[i] (last element in current heap).
Reduce heap size by 1 (i).

Heapify Again:
Call heapify(0, i) to restore max-heap property.

Repeat Until Done:
Continue until only one element remains in the heap (at index 0).

The array is now fully sorted.

Example:
Input → [2, 8, 5, 3, 9, 1]
Max Heap → [9, 8, 5, 3, 2, 1]
After swaps → [1, 2, 3, 5, 8, 9]

⏱️ Complexity

Time:

Building heap: O(n)
Extracting + heapify: O(n log n)
Total: O(n log n)

Space: O(1) → in-place, no extra array.

Stable: ❌ No (relative order of equal elements may change due to swaps).

✅ Notes:
Works on any data range, not just limited integers.
Useful when in-place sorting with guaranteed O(n log n) is needed.
Not as cache-friendly as merge sort or quicksort in practice, but great for worst-case guarantees.

👉 type: Min-Heap python
Understanding Python Min-Heap with heapq

✨ Core Idea
Python’s heapq module implements a min-heap, where the smallest element is always at the root.

heappush → inserts an element while maintaining heap property. After each push, the heap rearranges itself so the smallest element is at the root.
heappop → removes the smallest element (root). Heap automatically reorders → new root = next smallest
Accessing heap[0] → always gives the smallest element.

Check root:
h[0] → smallest element in the heap.

⏱️ Complexity

Time:
heappush → O(log n)
heappop → O(log n)

Space:
O(n) → stores all elements

Stable: ❌ (heap order is not stable, only heap property is guaranteed)

👉 type: Max-Heap python
Understanding Python Max-Heap with heapq (using negation)

✨ Core Idea
Python’s heapq module only provides a min-heap, but we can simulate a max-heap by pushing negative values:

heappush → inserts an element while maintaining heap property. Push negative values so the largest original number becomes the smallest in the heap.
heappop → removes the smallest element (which corresponds to the largest original number).
Accessing heap[0] → gives the largest original element when negated.

Check root:
-x[0]  # largest element

⏱️ Complexity

Time:

heappush → O(log n)
heappop → O(log n)

Space:
O(n) → stores all elements

Stable: ❌ (heap order is not stable, only heap property is guaranteed)

👉 Type: Divide & Conquer (Binary Search)

✅ Key Idea
Keep halving the search range until the target is found (or range becomes empty).

At each step, check the middle element:
If target == middle → found.
If target < middle → discard right half.
If target > middle → discard left half.

✨ Flashcard:
"Look at the middle → equal? done. smaller? go left. larger? go right → repeat until found or empty."

⚡ Full Approach

Steps:
Initialize:
left = 0, right = len(arr) - 1 (full range).
While left <= right:
Compute middle: mid = (left + right) // 2.
Compare arr[mid] with target:
If target > arr[mid] → search right half (left = mid + 1).
If target < arr[mid] → search left half (right = mid - 1).
Else return mid (found).
If loop ends without finding, return -1.

Base Case:
If array length is 0 → return -1 immediately.

⏱️ Complexity

Time:

Best: O(1) (target is at the middle on first check).
Average: O(log n).
Worst: O(log n).

Space:
O(1) → iterative implementation.

Stable:
Not applicable (search algorithm, not sorting).

✅ Notes:

Works only on sorted arrays.
Extremely efficient compared to linear search.
Commonly used in interviews and competitive programming.

👉 (Binary Search – Recursive)

✅ Key Idea
Recursively halve the search range until the target is found (or range becomes empty).

At each step:
If target == middle → found.
If target < middle → search left half.
If target > middle → search right half.

✨ Flashcard:
"Check the middle → equal? return index. smaller? search left. larger? search right → recurse until found or empty."

⚡ Full Approach

Steps:
Initialize: call recursive function with full range left=0, right=len(arr)-1.

Base Case:
If left > right, the search range is invalid → return -1.

Recursive Step:
Compute mid = (left + right) // 2.
If arr[mid] == target → return mid.
If target > arr[mid] → recurse into right half: binary_search_recursive(arr, target, mid+1, right).
If target < arr[mid] → recurse into left half: binary_search_recursive(arr, target, left, mid-1).

Finish: When recursion finds the element, it bubbles back with the index, else ends with -1.

⏱️ Complexity

Time:
Best: O(1) → target at middle initially.
Average: O(log n).
Worst: O(log n).

Space:
O(log n) (recursion stack).

✅ Notes:
Works only on sorted arrays.
Recursive approach is clean & elegant, but iterative version is more memory-efficient.

👉 Type: Divide & Conquer (Binary Search on Condition)

✅ Key Idea
Instead of searching for an exact value in an array, we search for a number in a range that satisfies a given condition (is_correct(n) == 0).
At each step:

If is_correct(mid) > 0 → current mid is too big, move left.
If is_correct(mid) < 0 → current mid is too small, move right.
If is_correct(mid) == 0 → condition satisfied → return mid.

✨ Flashcard
"Check middle → condition >0? go left → condition <0? go right → condition ==0? done → repeat until found or range empty."

⚡ Full Approach

Initialize:
left = start of range
right = end of range
Loop:
While left <= right:
Compute middle: mid = (left + right) // 2
Evaluate condition: result = is_correct(mid)

Move range based on result:

result > 0 → too big → right = mid - 1
result < 0 → too small → left = mid + 1
result == 0 → target found → return mid

Finish:

If no value satisfies the condition → return -1

⏱️ Complexity
Time: O(log(range)) → range halves each step
Space: O(1) → iterative

✅ Notes:
Useful when you don’t know the exact target but have a monotonic condition (is_correct(n) increases/decreases).
Very common in search problems in interviews (e.g., min/max problems, thresholds, or finding boundaries).

👉 Type: Divide & Conquer (Binary Search on Tree)

✅ Key Idea
Use the BST property (left < root < right) to decide where to search.
At each step:
If target > root.val → move right.
If target < root.val → move left.
If target == root.val → found, return True.
If root is None → reached an empty subtree, return False.

✨ Flashcard
"Check node → target bigger? go right → target smaller? go left → equal? found → None? not found."

⚡ Full Approach

Initialize:
Start from the root node.

Recursive Steps:
If root is None → return False (empty tree).

Compare target with root.val:
If target > root.val → search in right subtree.
If target < root.val → search in left subtree.
If target == root.val → return True.

Finish:
If recursion hits a None node → return False.
Otherwise, return True when found.

⏱️ Complexity

Time: O(h) → where h = height of the tree.
Worst case (unbalanced tree): O(n).
Best case (balanced tree): O(log n).

Space: O(h) → due to recursion stack (same worst/best cases).

✅ Notes:

This is the tree-version of binary search.
Can also be implemented iteratively to save recursion stack space.
Works only because it’s a Binary Search Tree (BST), not just any binary tree.


👉 Type: Divide & Conquer (Binary Search on Tree + Node Replacement)

✅ Key Idea
Use the BST property (left < root < right) to guide insertion, search, and deletion.
For removal, handle 3 cases:

Node has no child → just remove.
Node has one child → replace node with that child.
Node has two children → replace node with inorder successor (smallest value in right subtree), then remove successor.
Inorder traversal ensures values come out sorted.

✨ Flashcard
"Insert → left if smaller, right if bigger.
Remove → leaf? cut it. one child? link child. two children? swap with inorder successor → delete successor."

⚡ Full Approach

Insert:
If tree is empty → create new node.
If val < root.val → insert left.
If val > root.val → insert right.
Return updated root.

Remove:
If tree is empty → return None.
Search for node (go left or right using BST property).

When found:
No child → return None.
One child → return that child.

Two children →
Find min node in right subtree (min_value_node).
Replace current node value with it.
Recursively delete that min node from right subtree.

Inorder Traversal:
Traverse left subtree.
Visit root.
Traverse right subtree.
➡️ Produces sorted order of BST values.

⏱️ Complexity

Insert:
Time → O(h)
Space → O(h) recursion stack

Remove:
Time → O(h) (finding node + possible inorder successor)
Space → O(h) recursion stack

Inorder Traversal:
Time → O(n) (visits every node once)
Space → O(h) recursion depth

h = height of tree

Worst case (unbalanced, like a linked list): O(n)

Best case (balanced): O(log n)

✅ Notes

This is the classic BST delete algorithm.
inorder_traversal is a good test: after every insertion/deletion, it prints nodes in sorted order.
For balanced performance, a self-balancing BST (AVL, Red-Black Tree) is preferred.

👉 Type: Divide & Conquer (Tree Traversals - DFS)

✅ Key Idea
Traversal means visiting all nodes in a specific order.

Inorder (L → Root → R): Sorted order in BST.
Preorder (Root → L → R): Root first → useful for copying/serializing tree.
Postorder (L → R → Root): Root last → useful for deleting/freeing tree.
Reverse Inorder (R → Root → L): Largest first → descending order in BST.

✨ Flashcard
"Inorder = Sorted BST.
Preorder = Root first.
Postorder = Root last.
Reverse Inorder = Largest first."

⚡ Full Approach

1. Inorder Traversal

Traverse left subtree.
Visit root.
Traverse right subtree.
➡️ Gives ascending order in BST.

2. Preorder Traversal

Visit root.
Traverse left subtree.
Traverse right subtree.
➡️ Root comes first (good for building/serializing).

3. Postorder Traversal

Traverse left subtree.
Traverse right subtree.
Visit root.
➡️ Root comes last (good for deletion).

4. Reverse Inorder Traversal

Traverse right subtree.
Visit root.
Traverse left subtree.
➡️ Gives descending order in BST.

⏱️ Complexity (for all 4 traversals)

Time: O(n) → Each node visited once.
Space: O(h) recursion depth (stack).
Worst case (unbalanced): O(n).
Best case (balanced): O(log n).

✅ Notes

Inorder = useful for sorted output.
Preorder = useful for building/serializing trees.
Postorder = useful for freeing/deleting trees.
Reverse Inorder = useful for descending order or finding largest k elements.

👉 Type: Tree Traversal – BFS (Level Order Traversal)

✅ Key Idea
BFS visits nodes level by level from top to bottom, left to right.
It uses a queue to keep track of nodes at the current level while adding their children for the next level.

✨ Flashcard
"BFS = Level Order.
Use queue.
Process all nodes at one level before moving to the next."

⚡ Full Approach

Start with the root.
If root exists, push it into the queue.

While the queue is not empty:
Count current queue size → this represents all nodes at the current level.

Process each node (pop from queue, print value).
Push children (left first, then right) into the queue.

After finishing one level, increase the level counter.

➡️ This ensures nodes are printed level by level.

⏱️ Complexity

Time: O(n) → Every node is visited once.
Space: O(w) → where w = max width of tree (worst case O(n) if tree is very wide).

✅ Notes

BFS is best when you want the shortest path, or to process nodes level by level.
DFS (inorder, preorder, postorder) goes deep before wide, but BFS goes wide before deep.
Commonly used in shortest path algorithms, network spread problems, or level order printing of trees.

✅ Example Tree
        1
       / \
      2   3
     / \   \
    4   5   6

🔎 BFS Execution

Step 1: Start with root → queue = [1]

level = 0
Process node 1
Print 1

Add 2 and 3 → queue = [2, 3]

👉 Output:
level: 0
1

Step 2: queue = [2, 3]
level = 1
Process nodes 2, 3
Print 2 3

Add children: 2 → (4, 5) and 3 → (6)

queue = [4, 5, 6]

👉 Output:

level: 1
2
3

Step 3: queue = [4, 5, 6]

level = 2
Process nodes 4, 5, 6
Print 4 5 6

None of them has children → queue becomes empty.

👉 Output:

level: 2
4
5
6

✅ Final Output
level: 0
1
level: 1
2
3
level: 2
4
5
6

👉 Type: Tree Traversal – DFS with Backtracking

✅ Key Idea

Use DFS to explore all root-to-leaf paths, but backtrack whenever a path is invalid (e.g., encounters a node with val = 0).
Keep a path list to track the current traversal.
If the path reaches a valid leaf → return True.
If blocked → remove the node from path (backtracking).

✨ Flashcard

"DFS + backtracking → go deep, record path, undo if blocked, stop at valid leaf."

⚡ Full Approach

Base Case:
If root is None or root.val == 0 → cannot proceed → return False.

Include current node in path:
path.append(root.val)

Leaf Check:
If root.left is None and root.right is None → valid leaf → return True.

Recursive DFS:
Explore root.left → if it returns True → path is valid, propagate True.
Explore root.right → same logic.

Backtracking:

If neither left nor right returns True, remove current node from path → path.pop()
Return False to indicate no valid leaf along this path.

➡️ This ensures only valid paths are stored and blocked paths are discarded.

⏱️ Complexity

Time: O(n) → every node is visited once.
Space: O(h) → recursion stack + path list (h = tree height).

✅ Notes

Backtracking is useful for path-finding problems in trees, graphs, or grids.
DFS goes deep first, and backtracking undoes steps if a path fails.
canReachLeaf is a simpler version: checks existence without storing path.
leafPath stores one valid path using backtracking.

👉 Type: Heap Data Structure – Min-Heap Implementation

✅ Key Idea

A binary heap is a complete binary tree stored in an array.
Parent at i → left child 2*i, right child 2*i+1.
Min-heap property: every parent ≤ its children.

Operations:

push(val): Insert new value → bubble it up until parent is smaller.
pop(): Remove root (smallest) → move last element to root → percolate down.
heapify(arr): Build heap in O(n) by percolating down from last parent.

✨ Flashcard

"Min-heap → array-based tree where parent ≤ children. Push = bubble up. Pop = percolate down."

⚡ Full Approach

Heap Representation
Use an array with a dummy -inf at index 0 to simplify math.
Real elements start at index 1.

Push (Insert)
Append new value at the end.
Bubble up while parent > child.

Pop (Extract Min)
Save root (min).
Move last element to root.

Percolate down: swap with smaller child until heap property is restored.

Heapify (Build Heap)
Start from last parent → percolate down each node.
Runs in O(n).

Peek
Return root (min) without removing.

Helper: _percolate_down(i)
While left child exists:
Pick smaller of left/right child.
If child < current, swap and continue.
Else stop (heap property satisfied).

➡️ This ensures both insertions and deletions maintain O(log n) time complexity.

⏱️ Complexity

Push: O(log n)
Pop: O(log n)
Peek: O(1)
Heapify: O(n)
Space: O(n)

✅ Notes

The dummy -inf at index 0 makes parent/child indexing simpler.
Good for priority queues, scheduling, graph algorithms (Dijkstra, Prim).
This is a min-heap. A max-heap just flips the comparison signs.
